# Aprendizaje no supervisado

En unsupervised learning no tenemos variable respuesta.

Necesitamos agrupar registros que se parecen entre sÃ­, aunque no me vaya a decir quÃ© son realmente. 

## Tipos
**Clustering**

* K-means.
* Hierarchical Clustering.
* Probabilistic Clustering.

**Data Compression**

* Principal Component Analysis.
* Singular Value Decomposition (u otras factorizaciones de matrices).
* t-Distributed Stochastic Neighbor Embedding. 

**Unsupervised Deep Learning**

* Autoencoders.
* Anomaly detection.


##Conceptos 

### MÃ©trica (o distancia)

Para agrupar, necesitamos definir distancias. ImportantÃ­simo en aprendizaje supervisado. 

Una mÃ©trica ğ‘‘:ğ‘‹Ã—ğ‘‹â†’ (0,Infinito)  es una funciÃ³n que satisface las siguientes condiciones para cada ğ‘¥, ğ‘¦âˆˆğ‘‹:

*ğ‘‘ ğ‘¥,ğ‘¦ â‰¥0, no negativa.

*ğ‘‘ ğ‘¥,ğ‘¦ =0âŸºğ‘¥=ğ‘¦, identidad indecirnible.

*ğ‘‘ ğ‘¥,ğ‘¦ =ğ‘‘ ğ‘¦,ğ‘¥ , simetrÃ­a.

* ğ‘‘ ğ‘¥,ğ‘§ â‰¤ğ‘‘ ğ‘¥,ğ‘¦ + ğ‘‘ ğ‘¦,ğ‘§ , desigualdad triangular.

Tipos:

* Distancia euclideana.
* Distancia de Minkowski.
* Distancia de Manhattan.
* Distancia de Levenshtein.
* Distancia del infimo y supremo.

### Scaling

Tenemos un dataset con dos variables.

- Pesos: que va de 0 a 120. 
- NÃºmero de hijo: que va de 0 a 10. 

Si yo eso lo represento en un plano X,Y, me salen que un tÃ­o que tiene 60 k y 0 hijos se parece mucho a otro que pesa 60 y tiene 10, Hay que normalizar eso, para que todas las variables estÃ©n entre 0 y 1. 

**MÃ©todos de normalizaciÃ³n**

Existen varias formas de normalizar los valores de las variables. Uno es la estandarizaciÃ³n de la escala completa de todos los valores de las variables, es decir, que  ğ‘¥ ğ‘– âˆˆ 0,1 , conocida como min-max normalization, y se obtiene aplicando la siguiente transformaciÃ³n:

![f1](media/formula1.PNG) 

TambiÃ©n puede usarse la que seguramente muchos conocen:

![f1](media/formula2.PNG) 

### N/A

Como tenemos que calcular distancias, quÃ© hacemos con valores queno tienen valor para una variable. 

Lo mejor es asignarle la **moda**, no la **media**


## Â¿CÃ³mo determinar el nÃºmero de clusters?

### Elbow Method

El total within-cluster sum of square (wss) cuantifica quÃ© tan compacto es el clustering y queremos hacerlo tan pequeÃ±o como sea possible. Por tanto Podemos seguir el siguiente algoritmo para definir el nÃºmero Ã³ptimo de clusters:

* Calcula el algoritmo de clustering (por ejemplo, k-means clustering) para diferentes valores de k.
* Para cada k, calcula el total within-cluster sum of square (wss)
* Realiza un plot de la curva wss respect al nÃºmero de clusters k.
* La ubicaciÃ³n de una pequeÃ±a curva (un codo, elbow) en el plot, es generalmente considerada como un indcador apropiado del nÃºmero de clusters.

![codo](https://www.researchgate.net/profile/Chirag_Deb/publication/320986519/figure/fig8/AS:560163938422791@1510564898246/Result-of-the-elbow-method-to-determine-optimum-number-of-clusters.png)

### Average Silhouette Method

* En resumen, el average silhouette method se enfoca en medir la calidad de un clustering. Esto es, determina quÃ© tan bien se encuentra cada objeto dentro de su grupo.
* Un gran average silhouette indica un buen clustering
* El average silhouette method calcula la silueta promedio de observaciones para diferentes valores de k.
* El nÃºmero Ã³ptimo de clusters k es el que maximiza el average silhouette sobre un rango de posibles valores.
* Se define silhouette como:

![siluhouette](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_silhouette_analysis_004.png)

https://en.wikipedia.org/wiki/Silhouette_(clustering)

### Gap Statistic Method

* El Gap Statistic Method ha sido publicado por R. Tibshirani, G. Walther y T. Hastie (Standford University, 2001). PodrÃ­a aplicarse a otros mÃ©todos de clustering.
* Involucra simulaciones de Monte Carlo.
* Realiza Bootstraping para generar B copias del dataset de referencia.
* Es el mÃ¡s fundamentado matemÃ¡ticamente.

## Hierarchical Clustering

Un poco mÃ¡s difÃ­cil de entender. Henry nunca lo ha usado para datos reales

## Dimensionaliadades altas, como enfrentarnos a ellas. 

Por ejemplo, 


Como normalmente 



