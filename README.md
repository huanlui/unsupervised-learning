# Aprendizaje no supervisadoEn unsupervised learning no tenemos variable respuesta.Necesitamos agrupar registros que se parecen entre sÃ­, aunque no me vaya a decir quÃ© son realmente. ## Tipos**Clustering*** K-means.* Hierarchical Clustering.* Probabilistic Clustering.**Data Compression*** Principal Component Analysis.* Singular Value Decomposition (u otras factorizaciones de matrices).* t-Distributed Stochastic Neighbor Embedding. **Unsupervised Deep Learning*** Autoencoders.* Anomaly detection.##Conceptos ### MÃ©trica (o distancia)Para agrupar, necesitamos definir distancias. ImportantÃ­simo en aprendizaje supervisado. Una mÃ©trica ğ‘‘:ğ‘‹Ã—ğ‘‹â†’ (0,Infinito)  es una funciÃ³n que satisface las siguientes condiciones para cada ğ‘¥, ğ‘¦âˆˆğ‘‹:*ğ‘‘ ğ‘¥,ğ‘¦ â‰¥0, no negativa.*ğ‘‘ ğ‘¥,ğ‘¦ =0âŸºğ‘¥=ğ‘¦, identidad indecirnible.*ğ‘‘ ğ‘¥,ğ‘¦ =ğ‘‘ ğ‘¦,ğ‘¥ , simetrÃ­a.* ğ‘‘ ğ‘¥,ğ‘§ â‰¤ğ‘‘ ğ‘¥,ğ‘¦ + ğ‘‘ ğ‘¦,ğ‘§ , desigualdad triangular.Tipos:* Distancia euclideana.* Distancia de Minkowski.* Distancia de Manhattan.* Distancia de Levenshtein.* Distancia del infimo y supremo.### ScalingTenemos un dataset con dos variables.- Pesos: que va de 0 a 120. - NÃºmero de hijo: que va de 0 a 10. Si yo eso lo represento en un plano X,Y, me salen que un tÃ­o que tiene 60 k y 0 hijos se parece mucho a otro que pesa 60 y tiene 10, Hay que normalizar eso, para que todas las variables estÃ©n entre 0 y 1. **MÃ©todos de normalizaciÃ³n**Existen varias formas de normalizar los valores de las variables. Uno es la estandarizaciÃ³n de la escala completa de todos los valores de las variables, es decir, que  ğ‘¥ ğ‘– âˆˆ 0,1 , conocida como min-max normalization, y se obtiene aplicando la siguiente transformaciÃ³n:![f1](media/formula1.PNG) TambiÃ©n puede usarse la que seguramente muchos conocen:![f1](media/formula2.PNG) ### N/AComo tenemos que calcular distancias, quÃ© hacemos con valores queno tienen valor para una variable. Lo mejor es asignarle la **moda**, no la **media**## Â¿CÃ³mo determinar el nÃºmero de clusters?El total within-cluster sum of square (wss) cuantifica quÃ© tan compacto es el clustering y queremos hacerlo tan pequeÃ±o como sea possible. Por tanto Podemos seguir el siguiente algoritmo para definir el nÃºmero Ã³ptimo de clusters:* Calcula el algoritmo de clustering (por ejemplo, k-means clustering) para diferentes valores de k.* Para cada k, calcula el total within-cluster sum of square (wss)* Realiza un plot de la curva wss respect al nÃºmero de clusters k.* La ubicaciÃ³n de una pequeÃ±a curva (un codo, elbow) en el plot, es generalmente considerada como un indcador apropiado del nÃºmero de clusters.![codo](https://www.researchgate.net/profile/Chirag_Deb/publication/320986519/figure/fig8/AS:560163938422791@1510564898246/Result-of-the-elbow-method-to-determine-optimum-number-of-clusters.png)## Hierarchical ClusteringUn poco mÃ¡s difÃ­cil de entender. Henry nunca lo ha usado para datos reales## Dimensionaliadades altas, como enfrentarnos a ellas. Por ejemplo, Como normalmente 