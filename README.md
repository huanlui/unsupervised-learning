# Aprendizaje no supervisadoEn unsupervised learning no tenemos variable respuesta.Necesitamos agrupar registros que se parecen entre sÃ­, aunque no me vaya a decir quÃ© son realmente. ## Tipos**Clustering*** K-means.* Hierarchical Clustering.* Probabilistic Clustering.**Data Compression*** Principal Component Analysis.* Singular Value Decomposition (u otras factorizaciones de matrices).* t-Distributed Stochastic Neighbor Embedding. **Unsupervised Deep Learning*** Autoencoders.* Anomaly detection.##Conceptos ### MÃ©trica (o distancia)Una mÃ©trica ğ‘‘:ğ‘‹Ã—ğ‘‹â†’ (0,Infinito)  es una funciÃ³n que satisface las siguientes condiciones para cada ğ‘¥, ğ‘¦âˆˆğ‘‹:*ğ‘‘ ğ‘¥,ğ‘¦ â‰¥0, no negativa.*ğ‘‘ ğ‘¥,ğ‘¦ =0âŸºğ‘¥=ğ‘¦, identidad indecirnible.*ğ‘‘ ğ‘¥,ğ‘¦ =ğ‘‘ ğ‘¦,ğ‘¥ , simetrÃ­a.* ğ‘‘ ğ‘¥,ğ‘§ â‰¤ğ‘‘ ğ‘¥,ğ‘¦ + ğ‘‘ ğ‘¦,ğ‘§ , desigualdad triangular.Tipos:* Distancia euclideana.* Distancia de Minkowski.* Distancia de Manhattan.* Distancia de Levenshtein.* Distancia del infimo y supremo.### ScalingTenemos un dataset con dos variables.- Pesos: que va de 0 a 120. - NÃºmero de hijo: que va de 0 a 10. Si yo eso lo represento en un plano X,Y, me salen que un tÃ­o que tiene 60 k y 0 hijos se parece mucho a otro que pesa 60 y tiene 10, Hay que normalizar eso, para que todas las variables estÃ©n entre 0 y 1. **MÃ©todos de normalizaciÃ³n**![f1](media/formula1.png) ![f1](media/formula12png) ## K-MeansMuy fÃ¡cil de inteprestar. Muy utilizado.## Hierarchical ClusteringUn poco mÃ¡s difÃ­cil de entender. Henry nunca lo ha usado para datos reales## Dimensionaliadades altas, como enfrentarnos a ellas. Por ejemplo, Como normalmente 