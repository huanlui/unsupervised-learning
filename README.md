# Aprendizaje no supervisadoEn unsupervised learning no tenemos variable respuesta.Necesitamos agrupar registros que se parecen entre s√≠, aunque no me vaya a decir qu√© son realmente. ## Tipos**Clustering*** K-means.* Hierarchical Clustering.* Probabilistic Clustering.**Data Compression*** Principal Component Analysis.* Singular Value Decomposition (u otras factorizaciones de matrices).* t-Distributed Stochastic Neighbor Embedding. **Unsupervised Deep Learning*** Autoencoders.* Anomaly detection.##Conceptos ### M√©trica (o distancia)Para agrupar, necesitamos definir distancias. Important√≠simo en aprendizaje supervisado. Una m√©trica ùëë:ùëã√óùëã‚Üí (0,Infinito)  es una funci√≥n que satisface las siguientes condiciones para cada ùë•, ùë¶‚ààùëã:*ùëë ùë•,ùë¶ ‚â•0, no negativa.*ùëë ùë•,ùë¶ =0‚ü∫ùë•=ùë¶, identidad indecirnible.*ùëë ùë•,ùë¶ =ùëë ùë¶,ùë• , simetr√≠a.* ùëë ùë•,ùëß ‚â§ùëë ùë•,ùë¶ + ùëë ùë¶,ùëß , desigualdad triangular.Tipos:* Distancia euclideana.* Distancia de Minkowski.* Distancia de Manhattan.* Distancia de Levenshtein.* Distancia del infimo y supremo.### ScalingTenemos un dataset con dos variables.- Pesos: que va de 0 a 120. - N√∫mero de hijo: que va de 0 a 10. Si yo eso lo represento en un plano X,Y, me salen que un t√≠o que tiene 60 k y 0 hijos se parece mucho a otro que pesa 60 y tiene 10, Hay que normalizar eso, para que todas las variables est√©n entre 0 y 1. **M√©todos de normalizaci√≥n**Existen varias formas de normalizar los valores de las variables. Uno es la estandarizaci√≥n de la escala completa de todos los valores de las variables, es decir, que  ùë• ùëñ ‚àà 0,1 , conocida como min-max normalization, y se obtiene aplicando la siguiente transformaci√≥n:![f1](media/formula1.PNG) Tambi√©n puede usarse la que seguramente muchos conocen:![f1](media/formula2.PNG) ### N/AComo tenemos que calcular distancias, qu√© hacemos con valores queno tienen valor para una variable. Lo mejor es asignarle la **moda**, no la **media**# Algoritmos de clasificaci√≥n## K-MeansMuy f√°cil de inteprestar. Muy utilizado.Consiste en definir agrupaciones de modo que se minimice la variaci√≥n total dentro de la agrupaci√≥n.Esa variaci√≥n total se calcula usando el algoritmo de Hartigan-Wong. > la suma de las distancias al cuadrado de las distancias euclidianas entre los elementos y el centroide correspondiente.El centroide es la media de los puntos. ! [algoritmo](https://i.stack.imgur.com/r7WYA.png)Intento minimizar las distancias de los cuatro grupo (total within-cluster variation)![total within-cluster variation](https://www.saedsayad.com/images/Clustering_kmeans_c.png)Pero claro, para minimizar eso podr√≠a hacer 100 cluster si tengo 100 muestras. ### Algoritmo![kmeans](media/kmeans.PNG)## Hierarchical ClusteringUn poco m√°s dif√≠cil de entender. Henry nunca lo ha usado para datos reales## Dimensionaliadades altas, como enfrentarnos a ellas. Por ejemplo, Como normalmente 